{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "## Define tokenizer and stop words.\n",
    "\n",
    "def tokenize(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "\n",
    "stopWords = [\"。\", \"，\", \"？\"]\n",
    "\n",
    "## Split customer questions\n",
    "\n",
    "f = open('chatbot.txt', 'r')\n",
    "fileContent = f.read()\n",
    "contentSplit = fileContent.splitlines()\n",
    "clientQuestions = [x.split(\"):  \")[1] for x in contentSplit if (\"佩爱旗舰店\" not in x) and (\"):\" in x)]\n",
    "\n",
    "for i in range(len(clientQuestions)):\n",
    "    if \"[\" in clientQuestions[i] and \"]\" in clientQuestions[i]:\n",
    "        clientQuestions[i] = clientQuestions[i].split(\"[\")[0] + clientQuestions[i].split(\"[\")[1].split(\"]\")[1]\n",
    "        \n",
    "clientQuestions = [x for x in clientQuestions if (\"http\" not in x) and len(x) != 0]\n",
    "\n",
    "## TF-IDF process\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer = tokenize,\n",
    "                            stop_words = stopWords)\n",
    "matrix = vectorizer.fit_transform(clientQuestions)\n",
    "\n",
    "del fileContent, contentSplit\n",
    "\n",
    "## Features\n",
    "\n",
    "originalFeatures = vectorizer.get_feature_names()\n",
    "columnIndex = []\n",
    "columnNames = []\n",
    "\n",
    "for i in range(len(originalFeatures)):\n",
    "    if len(re.findall(r'[\\u4e00-\\u9fff]+', originalFeatures[i])) != 0:\n",
    "        columnIndex.append(i)\n",
    "        columnNames.append(originalFeatures[i])\n",
    "\n",
    "## Extract features        \n",
    "\n",
    "wordMatrix = np.array(matrix.toarray()).T[columnIndex]\n",
    "\n",
    "del matrix\n",
    "\n",
    "## Clusterring \n",
    "\n",
    "clusters = 50\n",
    "kmeans = np.array(KMeans(n_clusters=clusters).fit(wordMatrix.T[:10000]).labels_)\n",
    "\n",
    "## Get classes\n",
    "\n",
    "classes = []\n",
    "for i in range(clusters):\n",
    "    ind = np.where(kmeans == i)[0]\n",
    "    names = []\n",
    "    for j in ind:\n",
    "        names.append(clientQuestions[j])\n",
    "    classes.append(names)\n",
    "    \n",
    "for i in range(clusters):\n",
    "    f = open(\"Class_%d.txt\" %(i),'w')\n",
    "    newClass = map(lambda x:x+'\\n', classes[i])\n",
    "    f.writelines(newClass)\n",
    "    f.close()\n",
    "\n",
    "# features = [x for x in vectorizer.get_feature_names() if len(re.findall(r'[\\u4e00-\\u9fff]+', x)) != 0]\n",
    "# wordDf = pd.DataFrame(matrix.toarray(),\n",
    "#                       columns = vectorizer.get_feature_names())\n",
    "# wordVector = wordDf[features]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
